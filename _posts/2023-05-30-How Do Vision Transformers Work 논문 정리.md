---
key: jekyll-text-theme
title: How Do Vision Transformers Work? 논문 정리
excerpt: 'How Do Vision Transformers Work? 논문 읽고 정리하기'
tags: [AI, Computer Vision, Paper, Self-attention, Multi-head self-attention, ViT]
---

# How Do Vision Transformers Work? (ICLR 2022)

## Abstract

- Multi-head self-attentions (MSAs)는 loss landscape를 flatten시킴으로써 accuracy와 generalization을 향상시킴
  - 이러한 향상은 주로 long-range dependency가 아닌 데이터 특이성에 기인



---

## 작성 중

<br/>

> **Reference** 
>
> 
