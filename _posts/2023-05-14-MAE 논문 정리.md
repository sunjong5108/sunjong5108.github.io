---
key: jekyll-text-theme
title: MAE 논문 정리
excerpt: 'MAE 논문 읽고 정리하기'
tags: [AI, Computer Vision, Paper, Self-supervised learning, Pre-training, ViT]
---

# Masked Autoencoders Are Scalable Vision Learners (CVPR 2022)

## Abstract

- Masked autoencoder (MAE) → Computer vision을 위한 Scalable Self-supervised learner <br/>
  → input image를 무작위로 마스킹 $\Rightarrow$사라진 pixel들에 대해 재구성
- 2개의 핵심 설계
  1. 비대칭 encoder-decoder 구조
     - encoder: mask token 없이 patch의 visible subset에서만 동작
     - decoder: latent representation 및 mask token에서 원본 이미지를 재구성하는 경량 decoder
  2. 큰 비율로 input image 마스킹
     - 75 %로 만큼 마스킹 $\Rightarrow$의미있는 self-supervised task 도출
- MAE → large model를 효율적, 효과적으로 training<br/>
  $\Rightarrow$3배 또는  그 이상으로 training 가속화
- 일반화 성능이 좋고, Down-stream task에서 좋은 성능이 나옴


---

## Introduction

![image](https://github.com/sunjong5108/CAM-based_Flare_Removal_Network/assets/81843626/d4141bce-9385-4364-9c86-2c832b7d8091)

- 최근 model 능력 ↑, hardware 수준 ↑ $\Rightarrow$100만 장 이미지에 쉽게 overfitting

  - <span style='color:red'>→ 수 백만장의 data가 요구되는 문제점</span>
  - 이런 데이터의 요구 → NLP 분야에서의 Self-supervised pretraining 통해 해결
    - GPT(Autoregressive language modeling)
    - BERT(masked autoencoding)<br/>
      <span style='color:red'>$\Rightarrow$Data의 일부 제거하고 제거된 content를 예측하는 방법 학습</span>

- 저자들의 궁금증

  - Vision과 language 사이의 masked autoencoding 차이는 무엇일까?

    - 다음 관점에서 답을 찾으려 시도

      1. architecture 차이

         - Vision에서는 convolution network가 지배적
         - Convolution은 정해진 grid에서 동작
           mask token이나 positional embedding과 같은 'indicator'를 convolution network에 통합하는 것은 어려운 일<br/>

         <span style='color:red'>$\Rightarrow$ ViT가 등장하면서 해결됨</span>

      2. language와 vision사이의 information density 차이

         - language: semantic ↑, information-dense한 human-generated signal<br/>
           <span style='color:red'>→ 사람이 생성했기 때문에 그 자체로 의미가 많이 함축되어있음!</span>
           - 문장 내 없어진 단어를 예측하기 위해 모델을 훈련시킬 때 이 task는 정교한 language understanding을 유도
         - Image: heavy spatial redundancy를 가진 natural signal
           - missing patch는 parts, objects 그리고 scenes에 대한 약간의 high-level understanding과 함께 주변 patch들로부터 복원될 수 있음

    - 이런 차이점을 극복하고 유용한 feature들의 학습을 장려하기 위해서 많은 부분을 무작위로 마스킹함 <br/>
      <span style='color:red'>→ redundancy를 크게 줄이고 low-level image 통계 이상의 전체적인 이해가 필요한 어려운 self-supervisory task를 만듬</span>

      3. latent representation에서 input으로 되돌리는 autoencoder의 decoder는 text 재구성과 image 재구성 사이에서의 역할이 다름
         - Vision: decode가 pixel들을 재구성
           - <span style='color:red'>일반적인 recognition task보다 더 낮은 semantic level의 output이 나옴</span>
         - Language: decoder가 풍부한 semantic information을 포함하는 사라진 단어를 예측
           - BERT에서는 decoder가 사소할 수 있지만 (MLP), image의 경우 decoder 설계가 학습된 latent representation의 semantic level을 결정하는데 핵심적인 역할을 함


 ![image](https://github.com/sunjong5108/CAM-based_Flare_Removal_Network/assets/81843626/b07183f7-e67d-4df0-858b-47f4092ee2fc)

![image](https://github.com/sunjong5108/CAM-based_Flare_Removal_Network/assets/81843626/6e5eec77-39d5-44e7-af2a-ad003989b18f)

​		![image](https://github.com/sunjong5108/CAM-based_Flare_Removal_Network/assets/81843626/6ca84a79-55b7-4d45-8a0d-4b13a1107266)

- **3가지 관점들의 분석을 통해 vision representation learning을 위한 scalable 형태의 masked autoencoder을 제안**
- MAE: 입력 이미지에 무작위로 마스킹한 patch들을 재구성
  - 비대칭 encoder-decoder design
  - Encoder: mask token 없이 patch들 중 보이는 subset들에 대해서만 추측
  - Decoder: light-weight, mask token과 함께 latent representation에서 input을 재구성
  - <span style='color:red'>비대칭 encoder-decoder에서 mask token을 작은 decoder로 옮기면 계산량이 크게 감소</span>
  - masking ratio를 75 %로 높게 줌 **(win-win 시나리오)**
    - <span style='color:red'>왜? encoder가 patch들 중 적은 부분(25 %)에 대해서만 동작</span>
      - pretraining time 3배 이상 감소, memory 소비량도 비슷하게 줄어듬
      - MAE를 large model로 쉽게 확장 가능
- MAE 일반화 성능이 좋음
- ImageNet-1K에서의 ViT-Large/-Huge와 같은 Data-hungry 모델이 향상된 일반화 성능을 가지도록 학습 가능
- 다른 down-stream task에서도 좋은 성능을 가짐

---

## Method

### Masking

- ViT에서처럼 image → regular non-overlapping patches로 나누고 patch들 중 일부를 sampling 후 나머지 마스킹
  
- patch를 비복원 추출로 무작위 sampling <br/>이때, uniform distribution을 따름 **(random sampling)**
  
- random sampling은 높은 masking ratio를 가짐<br/>→ redundancy를 크게 제거, 어려운 task로 만듬
  
- Uniform distribution은 potential center bias를 예방<br/>→ 이미지 중심에 마스킹이 몰리는 것을 방지
  


### MAE Encoder

- encoder=ViT → 오직 unmasked patch들(visible)에 대해서만 적용
  - sparse input 때문에 효율적인 encoder 설계 가능

- MAE encoder는 오직 적은 양의 subset만 사용 (전체 patch의 25%)<br/>마스킹된 패치는 아예 제거됨 <span style='color:red'>→ mask token들 사용 X</span><br/>**→ 매우 큰 encoder들을 적은 계산과 메모리로 학습 가능**


### MAE Decoder

-  encoder visible patch들, mask token들로 구성된 token의 full set이 input으로 사용됨

-  각 mask token은 예측할 missing patch의 존재를 가리키는 공유되고 학습된 vector

-  이 full set의 모든 token에 positional embedding 추가 <br/><span style='color:red'>왜? mask token이 이미지에서 자신의 위치에 대한 정보를 가질 수 있도록 함</span>

-  MAE decoder는 image reconstruction을 수행하는 pre-training 중에만 사용
   -  오직 encoder만 recognition을 위한 image representation 생성하는데 사용됨

   -  decoder 구조는 encoder 설계와 독립적인 방법으로 유연하게 설계됨

   -  저자들은 encoder보다 좁고 얇은 decoder 사용해서 실험에 사용, encoder에 비해 token당 계산량이 10% 미만

-  이러한 비대칭 설계 덕분에 전체 token set은 light-weight decoder로만 처리되므로 pre-training 시간이 크게 단축

### Reconstruction Target

- MAE는 각 마스킹된 패치에 대한 pixel 값들을 예측함으로써 input을 재구성함
- decoder 출력의 각 요소는 하나의 patch들 포현하는 하나의 vector
- decoder의 마지막 layer는 출력 채널 수가 patch의 pixel값 수와 동일한 linear projection
  - decoder의 출력은 재구성된 이미지를 형성하도록 reshape됨

- loss function, pixel 공간에서 재구성된 이미지와 원본 이미지 간 MSE를 계산
  - BERT와 유사하게 마스킹된 patch에서만 loss 계산

- Reconstruction target 변형
  - 마스킹된 패치의 normalize된 pixel값을 reconstruction target으로 하는 변형을 연구
    - patch에 있는 모든 pixel의 평균과 표준편차를 계산하여 patch를 normalize하는데 사용

  - **Representation 품질 향상**

### Simple Implementation

1. 모든 input 패치에 대해 token 생성 (linear projection + positional encoding)
2. token의 list를 무작위로 섞고 list의 마지막 부분을 making ratio만큼 마스킹<br/><span style='color:red'>encoder를 위한 작은 subset를 생성, patch들을 비복원 추출하는 것과 동일</span>
3. encoding
4. encoding후 mask token의 list에 encoded patch의 list를 할당 → shuffle 해제 (무작위 shuffle 연산 반전) → 모든 token을 target에 맞춰 정렬
5. 4번에서의 full list + positional encoding → decoder에 적용


## Experimental results

- ImageNet Experiments
  ![image](https://github.com/sunjong5108/CAM-based_Flare_Removal_Network/assets/81843626/0449ade1-933f-4d3b-ad88-6bb89514458b)
  - Reconstruction target
    - normalization이 적용된 pixel 사용하면 정확도 향상
    - 패치별 normalization은 국부적으로(locally) 대비를 향상시킴
    - BEiT에서 사용하는 dVAE tokenizer는 **추가 데이터 (2억 5천만개의 이미지)**에 의존해야하는 pre-train 단계가 필요 **(비효율적)**
      - MAE는 이런 tokenizer 필요 없음! **(효율적, 장점)**

<br/>

> **Reference** 
>
> [https://arxiv.org/abs/2111.06377](https://arxiv.org/abs/2111.06377)/
