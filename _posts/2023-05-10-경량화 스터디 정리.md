---
key: jekyll-text-theme
title: 경량화 스터디 정리(20230510)
excerpt: '5월 10일에 진행된 경량화 스터디 정리하기'
tags: [AI, Efficient AI, Study, LLM, NLP, Optimization]
---

# Challenges on Optimization of LLM Inference (발표자: 네이버 클라우드 권세중님)

##  Introduction

- LLM 관심 ↑,  상용화하려는 목적 ↑, **최근에는 OpenAI의 ChatGPT가 등장하면서 거대 모델의 경량화에 대한 관심도가 증폭**
- <span style='color:red'>**중요 ★** 경량화, 최적화하기 위해서 모델에 대해 알아야함</span>

---

## CNN과 Transformer 계열 모델의 차이

![image](https://github.com/sunjong5108/CAM-based_Flare_Removal_Network/assets/81843626/de47b9df-98bf-4a53-9419-aef2831106e0)

- CNN: 자원 ↑, 시간 ↑ $\Rightarrow$ 성능이 계속 오르는 것이 아니라 Saturation 됨
- Transformer 계열의 모델들은 parameter수 ↑, 자원 ↑, 시간 ↑ $\Rightarrow$ **성능이 계속 좋아짐** (Scaling Law)
- Scaling Law를 막는 장애물? infra, infra의 효율성

---

## GPT 모델의 성공 요인

1) In-context Learning (No code AI)
   - Zero-shot, few-shot inference <span style='color:red'>→ 이전에는 task별로 학습을 다시 했어야했음, 근데 이제는 아님, 모델 하나만 잘 만들어서 여러 task들에 적용하는 것이 가능해졌다.</span>
   - Meta-Learning
2) Discontinuous Improvement
   - Generation 성능이 어느순간 확 좋아짐 (discontinuous하게)
   - **Big model에 열광하는 이유: **안되던게 되버리는 특이점이 와버림

---

## GPT3 VS. Instruct GPT

### GPT3는 확률 모델

- PROMPT가 주어지면 여러 문장을 만들어냄
  - PROMPT **"Explain the moon lading to a 6 years old in a few sentences."**
  - **"Explain"** 하라는 목적에 맞지 않는 결과들이 나올수 있음<br/><span style='color:red'>**왜? 확률 모델이기 때문에** </span>
    - 따라서 few-shot으로 더 힌트를 줘야함 (예제를 더 줘야함)
  - decoing에 대한 parameter 수가 많이 필요함
  - temperature 값에 따라서 대답이 유연해지거나 딱딱해지기도 함

### InstructGPT (ChatGPT)

- Few-shot를 많이 주지 않아도 원하는 방향으로 대답하기 시작 **(마치 사람과 대화하듯이)**

![image](https://github.com/sunjong5108/CAM-based_Flare_Removal_Network/assets/81843626/eb0ac79a-af6c-4b85-9f13-2526e0385ee5)

- 핵심? 데이터와 모델 사이즈 → 큰 모델이 더 잘 됨
- Metric의 함정
  - Accuracy가 좋은 model/adapter리 해서 이긴 것 X
  - 예시) ChatGPT VS. Grammarly <br/>각각 문법 오류 교체를 요구하면 Grammarly 같은 경우 문법적으로 오류가 있는 부분만 고치지만, ChatGPT의 경우 단어도 바꿔버림 때문에 Accuracy 측면에서는 ChatGPT $<$ Grammarly
    - 하지만 ChatGPT가 문법 오류 수정을 잘 못하는 것도 아님. 
    - Metric은 해당 부분에 대한 답을 정확히 맞춰야만 성능이 향상됨 때문에 GPT에 대한 성능이 떨어진다고 판단하기 어려움
  - ChatGPT가 인기 많은 이유는 말을 이쁘게 잘 만들어줄 뿐만 아니라 유려하기 떄문
- **하지만 ChatGPT는 175B model로 비용이 매우 비싸다.**

---

## Inference on Large Generative Model

![image](https://github.com/sunjong5108/CAM-based_Flare_Removal_Network/assets/81843626/56569f89-7ef4-48e1-8f3b-d543489fc772)

### Generative Model Workload

![image](https://github.com/sunjong5108/CAM-based_Flare_Removal_Network/assets/81843626/6ff239ae-a540-4aee-acc0-d3aa0471b8d8)

- Context summarization
  - BERT와 유사한 작업
  - Input size에 비례
  - Large batch size (seq_len $\times$ batch_size)
  - Computation bound → batch_size ↑, 학습 시간 ↑, 연산 시간 ↑
  - Higher weight-reusability = Higher utilization

![image](https://github.com/sunjong5108/CAM-based_Flare_Removal_Network/assets/81843626/1d066dcd-752f-41f8-9dce-e618b4a82010)

- Generative
  - output size에 비례
  - Small batch size (seq_len = 1, autoregressive)
  - Memory-bound
  - Lower weight-reusability = Lower utilization

- Transformer 연산량

  ![image](https://github.com/sunjong5108/CAM-based_Flare_Removal_Network/assets/81843626/5bcc8ed5-519a-4100-8479-0921b6b04149)

  - H: Hidden dim, L: Number of token, B: batch size
  - 4 Linear layers: $O(12BLH^2)$
  - Dot-product Attention: $O(2BHL^2)$
    - seq_len의 제곱으로 발생
      - 위와 같은 이유때문에 efficient transformer 연구로 발전
    - 하지만 모델이 커지면서 seq_len은 고정이고 Hidden dim이 증가하기 시작했고, 이런 이유때문에  attention의 비중은 상대적으로 낮아지고 Linear layer의 비중이 상대적으로 높아지게 되었음 <span style='color:red'>→ attention을 굳이 신경쓸 이유가 사라짐. $+$ Multi-GPU로 학습 및 추론이 가능해지면서 메모리 걱정도 사라지게 되었음</span>
  - 정리하면, 최근 Transformer 계열의 거대 모델들은 Linear layer에 집중하고 있음

- Transformer Workload

  ![image](https://github.com/sunjong5108/CAM-based_Flare_Removal_Network/assets/81843626/c17df915-7367-4094-88c6-5c04b1b16d42)

  - 175B $\times$ 2 byte (FP16) = 350GB **$\Rightarrow$ A100-80GB 5장이 필요한 수준**
  - 모델이 너무 크기 때문에 Data parallel 기법으로 해결이 불가능 **$\Rightarrow$ Tensor parallel 또는 pipeline이 요구된다.**

### Model Parallelsim VS. Data Parallelism

![image](https://github.com/sunjong5108/CAM-based_Flare_Removal_Network/assets/81843626/c1563138-361e-4507-a6e2-c064a036f2ec)

- Tensor Parallelism

  ![image](https://github.com/sunjong5108/CAM-based_Flare_Removal_Network/assets/81843626/1c1f2919-9146-4c95-9e8d-bdfbc5280615)

  - **결론: weight들이 나눠서 올라간다!**

    ![image](https://github.com/sunjong5108/CAM-based_Flare_Removal_Network/assets/81843626/add20842-e4f1-44b3-bf04-39d2aac694aa)

### LLM의 빠른 inference를 위해 무엇이 필요?

![image](https://github.com/sunjong5108/CAM-based_Flare_Removal_Network/assets/81843626/16f0f018-56fc-496d-a844-f63fe4c33bf8)

- Matmul (행렬곱) 연산량이 큼 **→ FLOPs의 영향이 엄청 크다는 의미**
- Utilization은 걱정 X
  - <span style='color:red'>왜? CNN이 아니니까!, CNN 처럼 다양한  operation이 필요하지 않고 dense 연산만 필요, 따라서 Linear layer 한 종류면 됨</span>
- DRAM 용량이 부족하면 Parallelism으로 인한 손실이 발생
- 큰 weight를 불러와서 쓰고 버리는 작업을 자주 해야하기 때문에 DRAM의 대역폭(B/W)가 매우 중요하다.
  - <span style='color:red'>왜? 큰 weight를 불러오는 작업을 하기 때문에 DRAM의 B/W에 모든게 bound되는 일이 벌어지기 때문</span>
  - 그럼 SRAM을 늘려서 해결이 가능할까? 아님! 따라서 SRAM보다 DRAM이 훨씬 중요

---

## LLM Inference는 어떻게 할 수 있을까?

- model 압축 방법들
  - weight size 줄이기
    - 기대효과? DRAM B/W의 효율성 증가
  - Operations(연산) 수 줄이기
  - Efficient Arithmetic Units (효율적인 산술 단위)
    - 기대효과?  Low power, Low area

- Compression method 종류?
  - Quantization
  - Pruning
    - SparseGPT
  - Low-rank decomposition
  - Knowledge Distillation
    - <span style='color:red'>Low-rank decomposition과 Knowledge Distillation은 모델이 너무 크기 때문에 재학습이 어려움</span>

### Quantization

![image](https://github.com/sunjong5108/CAM-based_Flare_Removal_Network/assets/81843626/fbf4d4f5-7e28-453a-8a2e-4c82f5115f08)

- Quantization(양자화)란?
  - 크고 연속적인 값의 집합($\mathbb R$)에서 더 작은 집합에서의 이산 값으로 입력 값을 매핑하는 과정
  - 더 간단하게 설명하면, Scaling factor를 뽑는 것 (대푯값)
    - 대표값을 뽑기 위해 평균을 취함
    - 평균을 내기 위해 데이터를 순회함
      ![image](https://github.com/sunjong5108/CAM-based_Flare_Removal_Network/assets/81843626/73a1d504-5477-431e-81f3-d2fcd2d612c8)
- 딥러닝에서의 quantization
  - Weight Quantization (static data): static data 압축이라 상대적으로 쉬움
  - Activation Quantization (Dynamic data): Weight Quantization보다 상대적으로 더 어려움, **왜? 예측이 더 힘들어서**<br/>ResNet과 같은 경우 값들이 깔끔하게 떨어져서 다루기 쉬웠지만, Trasnformer 계열의 모델들의 경우 값들의 범위도 넓어지고 값들이 깔끔하게 떨어지지 않아서 다루기 힘들다.\

- 어떻게 weight를 quantization할까?
  - Quantization format
    - Acceleration (H/W, Lib., ...)와 관련이 높음
    - Compression Ratio (압축 비율)
  - 학습 과정에서의 Quantization = How to recover degraded accuracy (성능하락 최대한 없이 만드는게 목적)
    - Quantization-aware Training (QAT)
    - Post-training Quantization (PTQ)
    - Data-free Quantization (DFQ)


### INT8

- INT8 연산자 사용하려면 activation quantization이 필수

- 그런데 Transformer랑 잘 맞지 않는다.

  - <span style='color:red'>왜? Transformer는 softmax, residual path에 의해 scaling이 다른 값을 만나는 경우도 존재하는데, 이는 softmax와 residual path가 값을 띄우는 성질을 가지기 때문이다. (INT8 적용에 큰 문제)</span>

  ![image](https://github.com/sunjong5108/CAM-based_Flare_Removal_Network/assets/81843626/2494ce96-efa9-4582-805c-17bf91e1dd29)

### INT8 + Transformer

![image](https://github.com/sunjong5108/CAM-based_Flare_Removal_Network/assets/81843626/0a142f62-ce31-4ebb-a5d9-3fb842165d3e)



- 위 그림에서 앞서 언급한 것과 같이 self-attention의 softmax 연산과 residual path에서 INT8 연산이 문제가 발생하기 때문에 해당 부분에 대해서 FP32로 바꾸는 옵션이 존재한다.

### INT8 + Transformer Inference

![image](https://github.com/sunjong5108/CAM-based_Flare_Removal_Network/assets/81843626/096db5bc-301b-4fcd-96bd-a292fb04ea88)

- 위 그림은 'Smooth Quant' 논문에서 제안하는 내용이다.
- 앞선 그림에서 self-attention 내 softmax 연산이 INT8연산에서 발생하는 문제를 방지하고자 dequantization을 해서 FP16으로 변환해준다. 또한 residual에서 합쳐지면서 작은 값들이 날라가는 문제를 막고자 이 부분에도 FP16으로 dequantization을 진행한다.
  - 이 논문에서는 충돌 문제가 적은 부분만 INT8을 적용하는게 핵심이다.
- 하지만 dequantization에도 단점이 존재한다.
  - type casting이 HW적으로 rich한 연산이기 때문에 overhead가 발생한다는 점이다.
  - **하지만 모델이 커지면 이런 overhead문제를 무시할 수 있게된다.**

### Quantization: 관건! 얼마나 분포를 잘 따라가냐! 또는 얼마나 분포를 잘 표현하냐! 

![image](https://github.com/sunjong5108/CAM-based_Flare_Removal_Network/assets/81843626/baf199ad-ff06-48b4-83b5-79feddfc7ad1)

- 연속적인 값들을 rounding하고, 범위를 넘어가는 값들은 이상치로 판단해 제거한다.

  - 위 그림에서 설명하는 quantization

  ![image](https://github.com/sunjong5108/CAM-based_Flare_Removal_Network/assets/81843626/b198e989-2ec5-472e-8e65-1147ba991b48)

  - 위 그림은 ResNet의 weight histogram과 trasformer의 weight histogram을 비교한다.
    - ResNet (위)의 경우 값의 범위가 넓지 않아 이상치를 자르는 값 범위 지정이 쉽지만 Transformer (아래)의 경우 값의 범위가 ResNet에 비해 날뛰고 다루기 힘들다는 것을 확인할 수 있다.
    - 즉, 앞서 말한 것처럼 Transformer의 activation quantization은 난이도가 높다.

### Lessons form QuantGPT

![image](https://github.com/sunjong5108/CAM-based_Flare_Removal_Network/assets/81843626/27e12ef4-2aca-4bce-82d7-c431709e9db9)

- Transformer 계열의 activation weight quantization은 weight 분포가 넓고 이상치가 많기 때문에 clipping을 잘하고, 그 범위를 좁히도록 Quantization aware training을 해야 함
- Quantization의 문제?
  - 값들이 다 뭉개진다! **→ 이를 방지해야 함**
  - GPT와 같은 모델은 autoregressive 방식, 결과가 나오고 그 결과가 다시 입력으로 들어가는 방식이기 때문에 error가 누적된다. 하지만 GPT는 확률모델이기 때문에 error 누적을 알아보기 힘들다.
    - 왜? error 누적을 알아보기 힘들까?
      - 확률 모델이기 때문에 확률이 조금씩 달라짐 + sampling해서 결과를 확인

### Lessons form ZeroQuant

![image](https://github.com/sunjong5108/CAM-based_Flare_Removal_Network/assets/81843626/b4571477-3315-4e45-8d3f-11b3c1f7161e)

- ZeroQuant에서도 앞서 언급한 내용들과 동일한 언급을 함
  - Weight가 너무 흩어져 있기 때문에 **압축이 어렵다!**

### Per-tensor, Per-token, Per-channel

![image](https://github.com/sunjong5108/CAM-based_Flare_Removal_Network/assets/81843626/18008c9e-cddd-43b8-b9fb-bc57451a0a51)

- 위 그림 (a)에서 필기한 내용과 같이 input을 통째로 잡아서 scaling facotr를 결정해 Quantization하면 편하긴 하지만 값이 뭉개지는 단점이 있다.
  - 이런 단점을 보완하기 위해 (b)처럼 token의 한 줄마다, channel의 한 줄마다 scaling factor를 결정해 Quantization하는 방법을 제안함

### Smooth Quant

![image](https://github.com/sunjong5108/CAM-based_Flare_Removal_Network/assets/81843626/f771818e-d06a-4574-9838-76dfb6c9d5c1)

![image](https://github.com/sunjong5108/CAM-based_Flare_Removal_Network/assets/81843626/3c4a4c91-3ad3-4948-80bb-f5bf06091e4b)

- Layer마다 scale을 잡도록해서 가속을 시킴 (아이디어)
  - 압축이 잘 됨
  - Overhead가 거의 없은 INT8 연산기를 돌릴 수 있음

### INT8 + Transformer 정리

- 단순한 Post-training Quantization 방법 사용
  - Quantization-aware Training은 사용 X, **왜? 너무 오래 걸린다!**
  - 복잡한 Minimum Reconstruction Error 방법 사용 X
- Fine-grained Quantization 적용 (ZeroQuant, LLM.int8)
  - Per-group / Per-channel Scaling Factors 사용
  - Dynamic Quantization for Activation
- SmoothQuant: Outlier의 Systematic Emergent에 집중
  - FP Decomposition (LLM.int8)
- 이런 방법들로 성능 하락 없이 1.5배 가속 + 2배정도 처리량이 증가

### Weight-only Quantization

- Activation 압축은 너무 어려워
  - 왜 압축하려고 헀을까?
    - INT8 연산기를 쓰려고
    - Image 처리하려면 activation의 양이 너무 크기 때문에 압축하려함
- **그런데, LLM의 activation은 아주 적은 양이고, 어차피 느려지는 원인은 Weight니까 Weight만 Quantization하자!**
  - 여러 논문들이 등장하기 시작
    - w4/a16 uniform quant
      - GLM-130B
      - GPTQ
      - On-the-fly dequantization and full-precision matrix multiplication
    - Weight-only Quantization w/ special library or H/W
      - GPU Kernel
      - New HW Design

### Quantization + Acceleration

![image](https://github.com/sunjong5108/CAM-based_Flare_Removal_Network/assets/81843626/52d4843b-7899-4d9a-b119-4a9223f6c1f1)

